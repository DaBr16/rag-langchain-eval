{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Fiqa Dataset\n",
    "In this Notebook I will analyse the Fiqa Dataset and transform it to apply some chunking strategies\n",
    "\n",
    "## Chunking Considerations\n",
    "- **What kind of Data are we working with?**\n",
    "    - size of content (long document like articles or book, or shorter content like tweets or messages)\n",
    "    - structure of content (plain text, code, tables,...)\n",
    "- **Which embedding model are we using and what chunk size does it perform optimally on?**\n",
    "- **How will the user queries look like?**\n",
    "    - Short and specific or long and complex\n",
    "- **How will the retrieved results be use?**\n",
    "    - e.g. sematic search, question answering, summarization\n",
    "\n",
    "### Papers\n",
    "- [Pinecone: Chunking Strategies](https://www.pinecone.io/learn/chunking-strategies/)\n",
    "- [Paper about Financial Report Chunking for RAG](https://arxiv.org/html/2402.05131v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enviroment variables\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# os.environ.get('OPENAI_API_KEY') d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "The Fiqa Dataset is a long text dataset. It is already divided into rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dabr/code/langchain-rag-eval/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiqa_corpus = load_dataset(\"explodinggradients/fiqa\", \"corpus\")\n",
    "\n",
    "print('Number of Rows (Chunks):' , len(fiqa_corpus['corpus']['doc']))\n",
    "\n",
    "length_of_rows = []\n",
    "for doc in fiqa_corpus['corpus']['doc']:\n",
    "    length_of_rows.append(len(doc))\n",
    "\n",
    "length_of_rows.sort(reverse=True)\n",
    "# len function counts number of characters\n",
    "print('Length of Rows: ', length_of_rows)\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.hist(length_of_rows, bins=50, color='blue', edgecolor='black')  \n",
    "\n",
    "plt.title('Distributions of Row Length FIQA Corpus')\n",
    "plt.xlabel('Length of Row')\n",
    "plt.ylabel('Number of Rows')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the size of the rows vary a lot and some are very long. So we need to get the full text and rechunk the data so we have a clean chunking strategy.\n",
    "\n",
    "*Maybe analyze also the content of some chunks or just view the [dataset](https://huggingface.co/datasets/explodinggradients/fiqa).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embed chunks and store into ChromaDB Vectorstore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "vectorstore = Chroma(persist_directory='../vectorstores/original_chunks', embedding_function=OpenAIEmbeddings(model='text-embedding-3-small'))\n",
    "corpus_len = len(fiqa_corpus['corpus'])\n",
    "\n",
    "for index, doc in enumerate(fiqa_corpus[\"corpus\"]):\n",
    "    # log.info(f\"Processing document {index + 1} of {corpus_len}\")\n",
    "    doc = Document(page_content=doc[\"doc\"], metadata={\"source\": \"local\", \"id\": index})\n",
    "    vectorstore.add_documents([doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Duration for embeddings: 282 min\n",
    "- Costs for embeddings: 0,20 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "First, I will convert the Fiqa Dataset I downloaded as csv to a text file, because for me it is easier to handle with text files.\n",
    "In the text file each row will be in an new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"../fiqa_dataset/corpus.txt\", \"w\") as my_output_file:\n",
    "    with open(\"../fiqa_dataset/corpus.csv\", \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../fiqa_dataset/corpus.txt\") as f:\n",
    "    fiqa_corpus = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed-size chunking\n",
    "- we decide how big each chunk should be and whether there should be an overlap between them to keep a little semantic context\n",
    "- computatially cheap and simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunksize 1000 and overlap 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\".\", \"\\n\", \"\\n\\n\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([fiqa_corpus])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Chunks: ', len(texts))\n",
    "\n",
    "length_of_chunks = []\n",
    "for doc in texts:\n",
    "    length_of_chunks.append(len(doc.page_content))\n",
    "\n",
    "length_of_chunks.sort(reverse=True)\n",
    "\n",
    "print('Length of Chunks: ', length_of_chunks)\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.hist(length_of_chunks, bins=50, color='blue', edgecolor='black')  \n",
    "\n",
    "plt.title('Fixed Size Chunking (1000)')\n",
    "plt.xlabel('Length of Chunks')\n",
    "plt.ylabel('Number of Chunks')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Create an instance of OpenAIEmbeddings\n",
    "embedding_function = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# Use the Chroma.from_documents class method with keyword arguments\n",
    "vectorstore_1000 = Chroma.from_documents(\n",
    "    texts, \n",
    "    embedding_function,\n",
    "    persist_directory='../vectorstores/fixed_size_1000'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- took only 15 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunksize 1500 and overlap 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\".\", \"\\n\", \"\\n\\n\"],\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([fiqa_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Chunks: ', len(texts))\n",
    "\n",
    "length_of_chunks = []\n",
    "for doc in texts:\n",
    "    length_of_chunks.append(len(doc.page_content))\n",
    "\n",
    "length_of_chunks.sort(reverse=True)\n",
    "\n",
    "print('Length of Chunks: ', length_of_chunks)\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.hist(length_of_chunks, bins=50, color='blue', edgecolor='black')  \n",
    "\n",
    "plt.title('Fixed Size Chunking (1000)')\n",
    "plt.xlabel('Length of Chunks')\n",
    "plt.ylabel('Number of Chunks')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dabr/code/langchain-rag-eval/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Create an instance of OpenAIEmbeddings\n",
    "embedding_function = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# Use the Chroma.from_documents class method with keyword arguments\n",
    "vectorstore = Chroma.from_documents(\n",
    "    texts, \n",
    "    embedding_function,\n",
    "    persist_directory='../vectorstores/fixed_size_1500'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunksize 2000 and overlap 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\".\", \"\\n\", \"\\n\\n\"],\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=400,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([fiqa_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Chunks: ', len(texts))\n",
    "\n",
    "length_of_chunks = []\n",
    "for doc in texts:\n",
    "    length_of_chunks.append(len(doc.page_content))\n",
    "\n",
    "length_of_chunks.sort(reverse=True)\n",
    "\n",
    "print('Length of Chunks: ', length_of_chunks)\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.hist(length_of_chunks, bins=50, color='blue', edgecolor='black')  \n",
    "\n",
    "plt.title('Fixed Size Chunking (1000)')\n",
    "plt.xlabel('Length of Chunks')\n",
    "plt.ylabel('Number of Chunks')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Create an instance of OpenAIEmbeddings\n",
    "embedding_function = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# Use the Chroma.from_documents class method with keyword arguments\n",
    "vectorstore_2000 = Chroma.from_documents(\n",
    "    texts, \n",
    "    embedding_function,\n",
    "    persist_directory='../vectorstores/fixed_size_2000'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First evaluation**\n",
    "- {'answer_relevancy': 0.3993, 'answer_correctness': 0.2876, 'context_recall': 0.4284, 'context_relevancy': 0.0443}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings(model='text-embedding-3-small'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.create_documents([fiqa_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_semantic = Chroma.from_documents(\n",
    "    docs, \n",
    "    embedding_function,\n",
    "    persist_directory='../vectorstores/semantic_chunks'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
